{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:41519</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8888/status' target='_blank'>http://127.0.0.1:8888/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>35.01 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:41519' processes=2 threads=4, memory=35.01 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.threaded\n",
    "import dask.multiprocessing\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=2,dashboard_address=':8888')\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path for modules\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "sys.path.insert(0,\"/mnt/meom/workdir/henelle/Notebooks/git/xscale\")\n",
    "import xscale\n",
    "import xscale.spectral.fft as xfft\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import numpy.ma as ma\n",
    "\n",
    "sys.path.insert(0,\"/mnt/meom/workdir/henelle/Notebooks/git/\")\n",
    "import myfunctions as mf\n",
    "\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib as mpl\n",
    "from datetime import date, datetime\n",
    "from xhistogram.xarray import histogram\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opening curl & strain files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 'JFM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tide\n",
    "\n",
    "dirvarT0     = '/mnt/meom/workdir/alberta/eNATL60/eNATL60-BLBT02-S/1h/ACO/'\n",
    "#curlfileT0   = dirvarT0 + 'eNATL60ACO-BLBT02_y2009m07-09_socurloverf10m_filt2T.nc' # JAS\n",
    "#strainfileT0 = dirvarT0 + 'eNATL60ACO-BLBT02_y2009m07-09_sostrainoverf10m_filt2T.nc' # JAS\n",
    "\n",
    "curlfileT0   = dirvarT0 + 'eNATL60ACO-BLBT02_y2010m01-03_socurloverf10m_filt2T.nc' # JFM\n",
    "strainfileT0 = dirvarT0 + 'eNATL60ACO-BLBT02_y2010m01-03_sostrainoverf10m_filt2T.nc' # JFM\n",
    "\n",
    "dscurlT0   = xr.open_mfdataset(curlfileT0,  combine='by_coords',parallel=True,chunks={'x':200,'y':200})\n",
    "dsstrainT0 = xr.open_mfdataset(strainfileT0,combine='by_coords',parallel=True,chunks={'x':200,'y':200})\n",
    "\n",
    "#curlT0   = dscurlT0.Curl_Tide_Filt # JAS\n",
    "curlT0   = dscurlT0.curl_Tide_Filt # JFM\n",
    "strainT0 = dsstrainT0.strain_Tide_Filt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160, 1191, 781)\n"
     ]
    }
   ],
   "source": [
    "# No tide\n",
    "\n",
    "dirvar00     = '/mnt/meom/workdir/alberta/eNATL60/eNATL60-BLB002-S/1h/ACO/'\n",
    "#curlfile00   = dirvar00 + 'eNATL60ACO-BLB002_y2009m07-09_socurloverf10m_filt2T.nc' # JAS\n",
    "#strainfile00 = dirvar00 + 'eNATL60ACO-BLB002_y2009m07-09_sostrainoverf10m_filt2T.nc' # JAS\n",
    "\n",
    "curlfile00   = dirvar00 + 'eNATL60ACO-BLB002_y2010m01-03_socurloverf10m_filt2T.nc' # JFM\n",
    "strainfile00 = dirvar00 + 'eNATL60ACO-BLB002_y2010m01-03_sostrainoverf10m_filt2T.nc' # JFM\n",
    "\n",
    "dscurl00   = xr.open_mfdataset(curlfile00,  combine='by_coords',parallel=True,chunks={'x':200,'y':200})\n",
    "dsstrain00 = xr.open_mfdataset(strainfile00,combine='by_coords',parallel=True,chunks={'x':200,'y':200})\n",
    "\n",
    "#curl00   = dscurl00.Curl_noTide_Filt # JAS\n",
    "curl00   = dscurl00.curl_noTide_Filt # JFM\n",
    "strain00 = dsstrain00.strain_noTide_Filt\n",
    "\n",
    "print(strain00.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening grid files\n",
    "\n",
    "dirgrid = '/mnt/meom/workdir/alberta/eNATL60/eNATL60-I/'\n",
    "gridfile = dirgrid+'mesh_hgr_eNATL60ACO_3.6.nc'\n",
    "dsgrid = xr.open_dataset(gridfile,chunks={'x':200,'y':200})\n",
    "\n",
    "lon = dsgrid.nav_lon\n",
    "lat = dsgrid.nav_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1191, 781)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choosing region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonmin, lonmax, latmin, latmax = -31,-28,30.6,33.6 # region A3\n",
    "#lonmin, lonmax, latmin, latmax = -31,-28,31.6,34.6 # region A1\n",
    "#lonmin, lonmax, latmin, latmax = -31,-28,33,36 # region B\n",
    "#lonmin, lonmax, latmin, latmax = -30,-27,30.6,33.6 # region A2\n",
    "#lonmin, lonmax, latmin, latmax = -35.0,-32.5,35.4,39.0 # region C\n",
    "#lonmin, lonmax, latmin, latmax = -34.0,-31.5,25.2,28.8 # region D\n",
    "\n",
    "region = 'A3'\n",
    "\n",
    "box = (lonmin, lonmax, latmin, latmax)\n",
    "\n",
    "domain = (box[0]<lon)*(lon<box[1])*(box[2]<lat)*(lat<box[3])\n",
    "where = np.where(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmin = where[0][0]\n",
    "jmax = where[0][-1]\n",
    "imin = where[1][0]\n",
    "imax = where[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 563 499 709\n"
     ]
    }
   ],
   "source": [
    "print(imin,imax,jmin,jmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainT0_filtbox = strainT0[:,jmin:jmax+1,imin:imax+1]\n",
    "strain00_filtbox = strain00[:,jmin:jmax+1,imin:imax+1]\n",
    "curlT0_filtbox   = curlT0  [:,jmin:jmax+1,imin:imax+1]\n",
    "curl00_filtbox   = curl00  [:,jmin:jmax+1,imin:imax+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining bins and weights for histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbin_width = 0.00625 # bin widths tested: 0.05 original, 0.025, 0.1, 0.0125\n",
    "cbin_width = 0.0125  # bin widths tested: 0.1 original, 0.05, 0.2, 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbins = np.arange(0.0,1.0, sbin_width)\n",
    "cbins = np.arange(-1.0,1.0, cbin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strain has 160 bins.\n",
      "Curl has 160 bins.\n"
     ]
    }
   ],
   "source": [
    "number_sbins = (1.0 - 0.0) / sbin_width\n",
    "number_cbins = (1.0 - (-1.0)) / cbin_width\n",
    "print('Strain has '+str(int(number_sbins))+' bins.')\n",
    "print('Curl has '+str(int(number_cbins))+' bins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain00_filtbox.name ='strain00'\n",
    "strainT0_filtbox.name ='strainT0'\n",
    "curl00_filtbox.name   ='curl00'\n",
    "curlT0_filtbox.name   ='curlT0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hypothesis: We are neglecting the variation of the area since the regions chosen are small enough that the variation\n",
    "### of the area is negligible in this case. \n",
    "hist_strain00_filtbox = histogram(strain00_filtbox,bins=[sbins])\n",
    "hist_strainT0_filtbox = histogram(strainT0_filtbox,bins=[sbins])\n",
    "hist_curl00_filtbox   = histogram(curl00_filtbox,bins=[cbins])\n",
    "hist_curlT0_filtbox   = histogram(curlT0_filtbox,bins=[cbins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('hist_strain00_filtbox'+region+'.npz', hist_strain00_filtbox = hist_strain00_filtbox)\n",
    "#np.savez('hist_strainT0_filtbox'+region+'.npz', hist_strainT0_filtbox = hist_strainT0_filtbox)\n",
    "#np.savez('hist_curl00_filtbox'+region+'.npz', hist_curl00_filtbox = hist_curl00_filtbox)\n",
    "#np.savez('hist_curlT0_filtbox'+region+'.npz', hist_curlT0_filtbox = hist_curlT0_filtbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('hist_strain00_bin'+region+season+'.npz', hist_strain00_bin = hist_strain00_filtbox.strain00_bin)\n",
    "np.savez('hist_strainT0_bin'+region+season+'.npz', hist_strainT0_bin = hist_strainT0_filtbox.strainT0_bin)\n",
    "np.savez('hist_curl00_bin'+region+season+'.npz', hist_curl00_bin = hist_curl00_filtbox.curl00_bin)\n",
    "np.savez('hist_curlT0_bin'+region+season+'.npz', hist_curlT0_bin = hist_curlT0_filtbox.curlT0_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalization n°2 with total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 211 179\n"
     ]
    }
   ],
   "source": [
    "# Convert tuple to int\n",
    "nt = int(''.join(map(str, strainT0_filtbox[:,0,0].shape)))\n",
    "ny = int(''.join(map(str, strainT0_filtbox[0,:,0].shape)))\n",
    "nx = int(''.join(map(str, strainT0_filtbox[0,0,:].shape)))\n",
    "print(nt,ny,nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81581040\n"
     ]
    }
   ],
   "source": [
    "vol = nt * ny * nx\n",
    "print(vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_strain00_norm2 = ( hist_strain00_filtbox / vol ) / sbin_width\n",
    "hist_strainT0_norm2 = ( hist_strainT0_filtbox / vol ) / sbin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_curl00_norm2 = ( hist_curl00_filtbox / vol ) / cbin_width\n",
    "hist_curlT0_norm2 = ( hist_curlT0_filtbox / vol ) / cbin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('hist_strain00_norm2'+region+season+'.npz', hist_strain00_norm2 = hist_strain00_norm2)\n",
    "np.savez('hist_strainT0_norm2'+region+season+'.npz', hist_strainT0_norm2 = hist_strainT0_norm2)\n",
    "np.savez('hist_curl00_norm2'+region+season+'.npz', hist_curl00_norm2 = hist_curl00_norm2)\n",
    "np.savez('hist_curlT0_norm2'+region+season+'.npz', hist_curlT0_norm2 = hist_curlT0_norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
